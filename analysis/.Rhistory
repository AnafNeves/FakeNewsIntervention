View(bfi)
df
#| message: false
#| warning: false
library(tidyverse)
library(easystats)
library(patchwork)
library(ggside)
library(summarydata)
#| message: false
#| warning: false
library(tidyverse)
library(easystats)
library(patchwork)
library(ggside)
library(modelsummary)
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("htmltools")
install.packages("bslib")
install.packages("rmarkdown")
install.packages("rmarkdown")
install.packages("installr")
library(installr)
updateR()
updateR()
remove.packages("htmltools")
remove.packages("bslib")
remove.packages("rmarkdown")
install.packages("htmltools", dependencies=TRUE)
install.packages("bslib", dependencies=TRUE)
install.packages("rmarkdown", dependencies=TRUE)
remove.packages("bslib")
remove.packages("rmarkdown")
install.packages("htmltools", dependencies=TRUE)
install.packages("bslib", dependencies=TRUE)
install.packages("rmarkdown", dependencies=TRUE)
install.packages("rmarkdown", dependencies=TRUE)
install.packages("htmltools", dependencies=TRUE)
install.packages("devtools")
install.packages("devtools")
install.packages(c("credentials", "bslib", "downlit", "httr2", "rlang"))
Sys.getenv("PATH")
install.packages(c("credentials", "bslib", "downlit", "httr2", "rlang"))
Sys.getenv("PATH")
#| code-fold: false
df <- read.csv("../data/rawdata.csv")
dfmist <- read.csv("../data/rawdata_mist.csv")
quarto::quarto_render("1_cleaning.qmd")
install.packages("quarto")
quarto::quarto_render("1_cleaning.qmd")
easystats::easystats_update()
remove.packages("easystats")
install.packages("easystats")
#| code-fold: false
df <- read.csv("../data/rawdata.csv")
dfmist <- read.csv("../data/rawdata_mist.csv")
df$Condition <-   ifelse(!is.na(df$Badnews_Questions_Duration), "BadNews",
ifelse(!is.na(df$Tetris_Questions_Duration), "Tetris", NA))
head(df$Condition)
modelsummary::datasummary_skim(df$Tetris_Questions_Duration)
datasummary_skim(Badnews_Questions_Duration ~ 1 | Condition, data = df)
modelsummary::datasummary_skim(Badnews_Questions_Duration ~ 1 | Condition, data = df)
modelsummary::datasummary_skim(
~ Badnews_Questions_Duration,
data = df[df$Condition == "BadNews", ]
)
modelsummary::datasummary_skim(
df$Badnews_Questions_Duration[df$Condition == "BadNews"]
)
Badnews <- df %>% select(Badnews_Questions_Duration)
library(dplyr)
Badnews <- df %>% select(Badnews_Questions_Duration)
modelsummary::datasummary_skim(Badnews)
Tetris <- df %>% select(Tetris_Questions_Duration)
modelsummary::datasummary_skim(Tetris)
setwd("C:/Users/Rob/OneDrive/Rob/PhD/Github/FakeNewsIntervention/analysis")
# Chunk 1
#| message: false
#| warning: false
library(tidyverse)
library(easystats)
library(patchwork)
library(ggside)
library(modelsummary)
config_modelsummary(startup_message = FALSE)
# Chunk 2
#| code-fold: false
df <- read.csv("../data/rawdata.csv")
dfmist <- read.csv("../data/rawdata_mist.csv")
# Chunk 3
#| code-fold: false
# Count missing values for each BFI10 item
BFI10_missing_values <- sapply(df[, paste0("BFI10_", 1:10)], function(x) sum(is.na(x)))
print(BFI10_missing_values)
# Chunk 4
#| code-fold: false
df$BFI_Agreeableness <- (df$BFI10_2 + (6-df$BFI10_7)) / 2
df$BFI_Extraversion <- (df$BFI10_6 + (6-df$BFI10_1)) / 2
df$BFI_Conscientiousness <- (df$BFI10_8 + (6-df$BFI10_3)) / 2
df$BFI_Neuroticism <- (df$BFI10_9 + (6-df$BFI10_4)) / 2
df$BFI_Openness <- (df$BFI10_10 + (6-df$BFI10_5)) / 2
# Chunk 5
bfi <- select(df, starts_with("BFI_"))
modelsummary::datasummary_skim(bfi)
# Chunk 6
#| code-fold: false
# Calculate mean and sd for each participant
df$GCBS_mean <- rowMeans(df[, paste0("GCBS15_", 1:15)], na.rm = TRUE)
df$GCBS_sd <- apply(df[, paste0("GCBS15_", 1:15)], 1, sd, na.rm = TRUE)
# Count missing values for each GCBS15 item
GCBS15_missing_values <- sapply(df[, paste0("GCBS15_", 1:15)], function(x) sum(is.na(x)))
print(GCBS15_missing_values)
# Range for each GCBS15 item
GCBS15_score_ranges <- sapply(df[, paste0("GCBS15_", 1:15)], function(x) range(x, na.rm = TRUE))
print(GCBS15_score_ranges)
# Overall range of scores across all GCBS15 items
GCBS15_overall_range <- range(as.matrix(df[, paste0("GCBS15_", 1:15)]), na.rm = TRUE)
print(GCBS15_overall_range)
# Chunk 7
GCBS <- select(df, starts_with("GCBS_"))
modelsummary::datasummary_skim(GCBS)
# Chunk 8
# Print responses for both ANES questions
print(df[, c("ANES_1", "ANES_2")])
ANES <- select(df, starts_with("ANES_"))
modelsummary::datasummary_skim(ANES)
# Chunk 9
# Select relevant columns
VSA <- select(df, starts_with("VSA_"))
# Inspect selected data
print(head(VSA))
# Apply modelsummary
modelsummary::datasummary_skim(VSA)
# Chunk 10
## does this give us useful data for both the pre- and post-intervention MOCRI, or do I need to split my coding to be between the bold and nonbold (as this is how the MOCRI is split pre- and post-)?
MOCRI <- select(df, starts_with("MOCRI_"))
modelsummary::datasummary_skim(MOCRI)
MIST <- select(df, starts_with("MIST_"))
MIST
dfmist
dfmist
head(df)
df
print(df)
df <- read.csv("../data/rawdata.csv")
dfmist <- read.csv("../data/rawdata_mist.csv")
df
dfmist
tidyr::separate(dfmist,col = "Condition", into = c("Pretest", "Posttest"),sep="_")
library(tidyverse)
library(patchwork)
library(dplyr)
library(tidyr)
df <- read.csv("../data/rawdata.csv") |>
mutate(Education = factor(Education,
levels = c("High School", "Bachelor", "Master", "Doctorate", "Other")))
dfmist <-  read.csv("../data/rawdata_mist.csv")
#MIST
# Split 'Item' column into 'QuestionType', 'Topic', and 'QuestionID'
dfmist <- dfmist %>%
separate(Item, into = c("MIST", "QuestionType", "Topic", "QuestionID"), sep = "_") %>%
select(-MIST)  # Drop the extra 'MIST' column if it's no longer needed
head(dfmist)
# Convert to wide format, pivoting based on Condition, RealFake, Topic, QuestionID, and Participant
df_wide <- dfmist %>%
pivot_wider(names_from = c(Condition, RealFake, Topic, QuestionID), values_from = MIST)
#MIST
# Split 'Item' column into 'RealFake', 'Topic', and 'QuestionID'
dfmist <- dfmist %>%
separate(Item, into = c("MIST", "RealFake", "Topic", "QuestionID"), sep = "_") %>%
select(-MIST)  # Drop the extra 'MIST' column if it's no longer needed
# Convert to wide format, pivoting based on Condition, RealFake, Topic, QuestionID, and Participant
df_wide <- dfmist %>%
pivot_wider(names_from = c(Condition, QuestionType, Topic, QuestionID), values_from = MIST)
colnames(dfmist)
dfmist <-  read.csv("../data/rawdata_mist.csv")
# Split 'Item' column into 'QuestionType', 'Topic', and 'QuestionID'
dfmist <- dfmist %>%
separate(Item, into = c("extra", "QuestionType", "Topic", "QuestionID"), sep = "_") %>%
select(-extra)  # Drop the extra 'extra' column if it's no longer needed
head(dfmist)
# Convert to wide format, pivoting based on Condition, RealFake, Topic, QuestionID, and Participant
df_wide <- dfmist %>%
pivot_wider(names_from = c(Condition, QuestionType, Topic, QuestionID), values_from = MIST)
head(df_wide)
write.csv(dfmist, "dfmist_longformat.csv", row.names = FALSE)
write.csv(dfmist_wide, "dfmist_wideformat.csv", row.names = FALSE)
write.csv(df_wide, "dfmist_wideformat.csv", row.names = FALSE)
# Check for duplicates in the long format dataframe
duplicate_rows <- dfmist %>%
group_by(Participant, QuestionID, Condition, Topic, QuestionType) %>%
filter(n() > 1)
# View the duplicates (if any)
print(duplicate_rows)
# Create a wide format dataframe
df_wide1 <- dfmist %>%
# Create a new column that combines all the necessary info for each unique column
mutate(QuestionLabel = paste0("Q", QuestionID, "_", QuestionType, "_", Topic, "_", Condition)) %>%
# Pivot the data to wide format, spreading the MIST score across the new columns
pivot_wider(names_from = QuestionLabel, values_from = MIST)
# View the resulting wide-format dataframe
head(df_wide1)
dfmist1 <- dfmist %>%
mutate(QuestionLabel = paste0("Q", QuestionID, "_", QuestionType, "_", Topic, "_", Condition))
# Step 2: Pivot the data to wide format
df_wide1 <- dfmist1 %>%
pivot_wider(names_from = QuestionLabel, values_from = MIST)
# Step 3: View the result to check the wide format
head(df_wide1)
rm(dfmist1)
rm(df_wide1)
library(tidyverse)
library(patchwork)
library(dplyr)
library(tidyr)
dfmist <-  read.csv("../data/rawdata_mist.csv")
dfmist <- dfmist %>%
separate(Item, into = c("extra", "QuestionType", "Topic", "QuestionID"), sep = "_") %>%
select(-extra)  # Drop the extra 'extra' column if it's no longer needed
# Convert to wide format, pivoting based on Condition, RealFake, Topic, QuestionID, and Participant
df_wide <- dfmist %>%
pivot_wider(names_from = c(Condition, QuestionType, Topic, QuestionID), values_from = MIST)
# Add binary scores for each question
dfmist <- dfmist %>%
mutate(across(contains("Pretest_") | contains("Posttest_"),
~ case_when(
str_detect(cur_column(), "real") & . >= 50 ~ 1,
str_detect(cur_column(), "fake") & . < 50 ~ 1,
TRUE ~ 0
),
.names = "{.col}_binary"))
# Add binary scores for each question
df_wide <- df_wide %>%
mutate(across(contains("Pretest_") | contains("Posttest_"),
~ case_when(
str_detect(cur_column(), "real") & . >= 50 ~ 1,
str_detect(cur_column(), "fake") & . < 50 ~ 1,
TRUE ~ 0
),
.names = "{.col}_binary"))
# 1. Overall accuracy rate per participant (binary)
dfmist_summary_participant <- dfmist %>%
rowwise() %>%
mutate(overall_accuracy = sum(across(ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(ends_with("_binary")))))
# 2. Accuracy rate per participant for Pretest
dfmist_summary_participant_pre <- dfmist %>%
rowwise() %>%
mutate(pretest_accuracy = sum(across(starts_with("Pretest_"), ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(starts_with("Pretest_"), ends_with("_binary")))))
rm(dfmist_summary_participant_post)
rm(dfmist_summary_participant)
# 1. Overall accuracy rate per participant (binary) in df_wide
df_wide_summary_participant <- df_wide %>%
rowwise() %>%
mutate(overall_accuracy = sum(across(ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(ends_with("_binary")))))
# 2. Accuracy rate per participant for Pretest in df_wide
df_wide_summary_participant_pre <- df_wide %>%
rowwise() %>%
mutate(pretest_accuracy = sum(across(starts_with("Pretest_"), ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(starts_with("Pretest_"), ends_with("_binary")))))
# 1. Overall accuracy rate per participant (binary) in df_wide
df_wide_summary_participant <- df_wide %>%
rowwise() %>%
mutate(overall_accuracy = sum(across(ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(ends_with("_binary")))))
# 2. Accuracy rate per participant for Pretest in df_wide
df_wide_summary_participant_pre <- df_wide %>%
rowwise() %>%
mutate(pretest_accuracy = sum(across(starts_with("Pretest_") & ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(starts_with("Pretest_") & ends_with("_binary")))))
# 3. Accuracy rate per participant for Posttest in df_wide
df_wide_summary_participant_post <- df_wide %>%
rowwise() %>%
mutate(posttest_accuracy = sum(across(starts_with("Posttest_") & ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(starts_with("Posttest_") & ends_with("_binary")))))
# 4. Effect size: difference between pretest and posttest accuracy
df_wide_summary_participant <- df_wide_summary_participant %>%
left_join(df_wide_summary_participant_pre %>%
select(Participant, pretest_accuracy), by = "Participant") %>%
left_join(df_wide_summary_participant_post %>%
select(Participant, posttest_accuracy), by = "Participant") %>%
mutate(effect_size = posttest_accuracy - pretest_accuracy)
# View the summary table
df_wide_summary_participant
# 1. Add overall accuracy rate per participant (binary)
df_wide <- df_wide %>%
rowwise() %>%
mutate(overall_accuracy = sum(across(ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(ends_with("_binary")))))
# 2. Add accuracy rate for Pretest (binary)
df_wide <- df_wide %>%
rowwise() %>%
mutate(pretest_accuracy = sum(across(starts_with("Pretest_") & ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(starts_with("Pretest_") & ends_with("_binary")))))
# 3. Add accuracy rate for Posttest (binary)
df_wide <- df_wide %>%
rowwise() %>%
mutate(posttest_accuracy = sum(across(starts_with("Posttest_") & ends_with("_binary")), na.rm = TRUE) / sum(!is.na(across(starts_with("Posttest_") & ends_with("_binary")))))
# 4. Calculate the effect size (difference between posttest and pretest accuracy)
df_wide <- df_wide %>%
mutate(effect_size = posttest_accuracy - pretest_accuracy)
# View the updated df_wide with new columns
head(df_wide)
write.csv(df_wide, "df_wide_summary.csv", row.names = FALSE)
df_wide <- df_wide %>%
mutate(across(ends_with("_binary"),
~ case_when(
. == 1 ~ abs(get(str_replace(cur_column(), "_binary", "")) - 50), # Confidence for correct answers
. == 0 ~ abs(get(str_replace(cur_column(), "_binary", "")) - 50) # Confidence for incorrect answers (same formula for now)
), .names = "{.col}_confidence"))
# Now df_wide should have new confidence columns based on the binary columns for each question
write.csv(df_wide, "df_wide_confidence.csv", row.names = FALSE)
df_wide_confidence <- df_wide %>%
# Pretest: Average confidence for correctly answered questions
mutate(
pretest_correct_confidence = rowMeans(select(., starts_with("Pretest") & ends_with("binary_confidence"))[df_wide_confidence[starts_with("Pretest") & ends_with("binary_confidence")] == 1], na.rm = TRUE),
# Pretest: Average confidence for incorrectly answered questions
pretest_incorrect_confidence = rowMeans(select(., starts_with("Pretest") & ends_with("binary_confidence"))[df_wide_confidence[starts_with("Pretest") & ends_with("binary_confidence")] == 0], na.rm = TRUE),
# Pretest: Overall average confidence for all pretest questions
pretest_overall_confidence = rowMeans(select(., starts_with("Pretest") & ends_with("binary_confidence")), na.rm = TRUE),
# Posttest: Average confidence for correctly answered questions
posttest_correct_confidence = rowMeans(select(., starts_with("Posttest") & ends_with("binary_confidence"))[df_wide_confidence[starts_with("Posttest") & ends_with("binary_confidence")] == 1], na.rm = TRUE),
# Posttest: Average confidence for incorrectly answered questions
posttest_incorrect_confidence = rowMeans(select(., starts_with("Posttest") & ends_with("binary_confidence"))[df_wide_confidence[starts_with("Posttest") & ends_with("binary_confidence")] == 0], na.rm = TRUE),
# Posttest: Overall average confidence for all posttest questions
posttest_overall_confidence = rowMeans(select(., starts_with("Posttest") & ends_with("binary_confidence")), na.rm = TRUE)
)
# Create the new columns in df_wide
df_wide_confidence <- df_wide %>%
# Pretest: Average confidence for correctly answered questions
mutate(
pretest_correct_confidence = rowMeans(select(., starts_with("Pretest") & ends_with("binary_confidence"))[df_wide[starts_with("Pretest") & ends_with("binary_confidence")] == 1], na.rm = TRUE),
# Pretest: Average confidence for incorrectly answered questions
pretest_incorrect_confidence = rowMeans(select(., starts_with("Pretest") & ends_with("binary_confidence"))[df_wide[starts_with("Pretest") & ends_with("binary_confidence")] == 0], na.rm = TRUE),
# Pretest: Overall average confidence for all pretest questions
pretest_overall_confidence = rowMeans(select(., starts_with("Pretest") & ends_with("binary_confidence")), na.rm = TRUE),
# Posttest: Average confidence for correctly answered questions
posttest_correct_confidence = rowMeans(select(., starts_with("Posttest") & ends_with("binary_confidence"))[df_wide[starts_with("Posttest") & ends_with("binary_confidence")] == 1], na.rm = TRUE),
# Posttest: Average confidence for incorrectly answered questions
posttest_incorrect_confidence = rowMeans(select(., starts_with("Posttest") & ends_with("binary_confidence"))[df_wide[starts_with("Posttest") & ends_with("binary_confidence")] == 0], na.rm = TRUE),
# Posttest: Overall average confidence for all posttest questions
posttest_overall_confidence = rowMeans(select(., starts_with("Posttest") & ends_with("binary_confidence")), na.rm = TRUE)
)
df_wide <- df_wide %>%
# Calculate the average confidence for pretest questions answered correctly
mutate(
pretest_correct_confidence = rowMeans(
select(., starts_with("Pretest") & ends_with("binary_confidence"))[
select(., starts_with("Pretest") & ends_with("binary_confidence")) == 1
], na.rm = TRUE
),
# Calculate the average confidence for pretest questions answered incorrectly
pretest_incorrect_confidence = rowMeans(
select(., starts_with("Pretest") & ends_with("binary_confidence"))[
select(., starts_with("Pretest") & ends_with("binary_confidence")) == 0
], na.rm = TRUE
),
# Calculate the average confidence for all pretest questions
pretest_overall_confidence = rowMeans(
select(., starts_with("Pretest") & ends_with("binary_confidence")), na.rm = TRUE
),
# Calculate the average confidence for posttest questions answered correctly
posttest_correct_confidence = rowMeans(
select(., starts_with("Posttest") & ends_with("binary_confidence"))[
select(., starts_with("Posttest") & ends_with("binary_confidence")) == 1
], na.rm = TRUE
),
# Calculate the average confidence for posttest questions answered incorrectly
posttest_incorrect_confidence = rowMeans(
select(., starts_with("Posttest") & ends_with("binary_confidence"))[
select(., starts_with("Posttest") & ends_with("binary_confidence")) == 0
], na.rm = TRUE
),
# Calculate the average confidence for all posttest questions
posttest_overall_confidence = rowMeans(
select(., starts_with("Posttest") & ends_with("binary_confidence")), na.rm = TRUE
)
)
df_wide <- df_wide %>%
# Calculate the average confidence for pretest questions answered correctly
mutate(
pretest_correct_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence"))[
, which(select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) == 1)
], na.rm = TRUE
),
# Calculate the average confidence for pretest questions answered incorrectly
pretest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence"))[
, which(select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) == 0)
], na.rm = TRUE
),
# Calculate the average confidence for all pretest questions
pretest_overall_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")), na.rm = TRUE
),
# Calculate the average confidence for posttest questions answered correctly
posttest_correct_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence"))[
, which(select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) == 1)
], na.rm = TRUE
),
# Calculate the average confidence for posttest questions answered incorrectly
posttest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence"))[
, which(select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) == 0)
], na.rm = TRUE
),
# Calculate the average confidence for all posttest questions
posttest_overall_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")), na.rm = TRUE
)
)
df_wide <- df_wide %>%
mutate(
pretest_correct_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Pretest") & ends_with("binary")) == 1, 1, 0),
na.rm = TRUE
),
pretest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Pretest") & ends_with("binary")) == 0, 1, 0),
na.rm = TRUE
),
pretest_overall_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")),
na.rm = TRUE
),
posttest_correct_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Posttest") & ends_with("binary")) == 1, 1, 0),
na.rm = TRUE
),
posttest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Posttest") & ends_with("binary")) == 0, 1, 0),
na.rm = TRUE
),
posttest_overall_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")),
na.rm = TRUE
)
)
df_wide <- df_wide %>%
mutate(
# Correct confidence scores (pretest)
pretest_correct_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Pretest") & ends_with("binary")) == 1, 1, 0),
na.rm = TRUE
),
# Incorrect confidence scores (pretest)
pretest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Pretest") & ends_with("binary")) == 0, 1, 0),
na.rm = TRUE
),
# Overall confidence (pretest)
pretest_overall_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")),
na.rm = TRUE
),
# Correct confidence scores (posttest)
posttest_correct_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Posttest") & ends_with("binary")) == 1, 1, 0),
na.rm = TRUE
),
# Incorrect confidence scores (posttest)
posttest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) *
ifelse(select(df_wide, starts_with("Posttest") & ends_with("binary")) == 0, 1, 0),
na.rm = TRUE
),
# Overall confidence (posttest)
posttest_overall_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")),
na.rm = TRUE
)
)
df_wide <- df_wide %>%
# Correct confidence scores for pretest (answered correctly)
mutate(
pretest_correct_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) *
(select(df_wide, starts_with("Pretest") & ends_with("binary")) == 1),
na.rm = TRUE
),
# Incorrect confidence scores for pretest (answered incorrectly)
pretest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")) *
(select(df_wide, starts_with("Pretest") & ends_with("binary")) == 0),
na.rm = TRUE
),
# Overall confidence scores for pretest (all answers, not based on correctness)
pretest_overall_confidence = rowMeans(
select(df_wide, starts_with("Pretest") & ends_with("binary_confidence")),
na.rm = TRUE
),
# Correct confidence scores for posttest (answered correctly)
posttest_correct_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) *
(select(df_wide, starts_with("Posttest") & ends_with("binary")) == 1),
na.rm = TRUE
),
# Incorrect confidence scores for posttest (answered incorrectly)
posttest_incorrect_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")) *
(select(df_wide, starts_with("Posttest") & ends_with("binary")) == 0),
na.rm = TRUE
),
# Overall confidence scores for posttest (all answers, not based on correctness)
posttest_overall_confidence = rowMeans(
select(df_wide, starts_with("Posttest") & ends_with("binary_confidence")),
na.rm = TRUE
)
)
